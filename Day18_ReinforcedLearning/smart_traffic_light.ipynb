{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d79446d-bdb0-447c-892c-014b98e8c1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully impoerting NumPy and random libraries\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "print(\"Sucessfully impoerting NumPy and random libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474f3ff9-b696-4a34-864a-e3227d495bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(random.choice([0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf8c92f3-7692-42f4-87e4-c5e18481399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "['GREEN', 'RED']\n"
     ]
    }
   ],
   "source": [
    "states = np.arange(5)\n",
    "actions = ['GREEN','RED']\n",
    "print(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3984d0ce-203c-4163-b091-093e30b7a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q =np.zeros((len(states),len(actions)))\n",
    "alpha =0.1 # Learning rate\n",
    "gama = 0.9 # Discount factor\n",
    "epsilon = 0.2 # Exploration rate\n",
    "episodes = 300 # Training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5c63d90-01cf-4030-b55d-15e395196f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Example (level=3, red): -10\n"
     ]
    }
   ],
   "source": [
    "def get_reward(states,action):\n",
    "    if 3 <= states < 5:  #heavy 3,4\n",
    "        if action == 'GREEN':\n",
    "            reward = 10 # clears congestion\n",
    "        else:\n",
    "            reward = -10 #create jams\n",
    "    elif states == 0:  # empty road\n",
    "        if action == 'RED':\n",
    "            reward =  5 # saves energy\n",
    "        else:\n",
    "            reward = -5 # wastes power\n",
    "    else:\n",
    "        reward = 1 # moderate\n",
    "    return reward\n",
    "\n",
    "print('Reward Example (level=3, red):', get_reward(3, 'RED'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fba473c-4fa5-441f-8291-06af64751710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current traffic: 1 → Next traffic: 0\n"
     ]
    }
   ],
   "source": [
    "def next_traffic(current,action):\n",
    "    if action == 'GREEN':\n",
    "        current += np.random.choice([-1, 0, 1])\n",
    "    else:\n",
    "        current -= np.random.choice([-1, 0, 1])\n",
    "    return int(np.clip(current, 0, 4) )\n",
    "\n",
    "current = np.random.randint(0, 5)\n",
    "print(f\"Current traffic: {current} → Next traffic: {next_traffic(current,'GREEN')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08dd8c73-4711-4c29-bc18-3b58c5aa7a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "for ep in range(episodes):\n",
    "    traffic = random.choice(states)\n",
    "    for _ in range(15): # steps per episode\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(actions)\n",
    "        else:\n",
    "            action = actions[np.argmax(Q[traffic])]\n",
    " \n",
    "        next_state = next_traffic(traffic, action)\n",
    "        reward = get_reward(next_state, action)\n",
    " \n",
    "        a = actions.index(action)\n",
    "        best_next = np.max(Q[next_state])\n",
    "        Q[traffic, a] += alpha * (reward + gama * best_next - Q[traffic, a])\n",
    "print(\"Training Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8a68328-8290-4008-808f-9dc6d7f1be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter starting traffic state (0–4):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting with state: 1\n",
      "Simulating for 10 steps:\n",
      "\n",
      "Step 1: State=1 -> Action=RED\n",
      "Step 2: State=0 -> Action=RED\n",
      "Step 3: State=0 -> Action=RED\n",
      "Step 4: State=0 -> Action=RED\n",
      "Step 5: State=1 -> Action=RED\n",
      "Step 6: State=2 -> Action=GREEN\n",
      "Step 7: State=1 -> Action=RED\n",
      "Step 8: State=2 -> Action=GREEN\n",
      "Step 9: State=1 -> Action=RED\n",
      "Step 10: State=1 -> Action=RED\n",
      "\n",
      "Simulation complete. Traffic Control finished.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    state = int(input('Enter starting traffic state (0–4): '))\n",
    "    if state < 0 or state > 4:\n",
    "        raise ValueError('Traffic level out of range!')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    state = 2\n",
    "    print('Defaut level set to 2')\n",
    "\n",
    "print(f'\\nStarting with state: {state}')\n",
    "print('Simulating for 10 steps:\\n')\n",
    "\n",
    "\n",
    "for step in range(10):\n",
    "    action = actions[np.argmax(Q[state])]\n",
    "    print(f'Step {step+1}: State={state} -> Action={action}')\n",
    "    state= next_traffic(state, action)\n",
    "\n",
    "print('\\nSimulation complete. Traffic Control finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c81032c-8871-4b06-806c-2bcfec58a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis ===\n",
      "- Does the system keep GREEN for heavy traffic?\n",
      "  * For heavy/very heavy (states 3 & 4), policy is not consistently GREEN.\n",
      "  * Q-values for heavy states: state 3 -> [20.281 24.725], state 4 -> [20.556 23.899]\n",
      "\n",
      "- Does it turn RED when the road is empty?\n",
      "  * For empty (state 0), policy is RED.\n",
      "  * Q-values for empty state: [19.758 24.455]\n",
      "\n",
      "- What happens when epsilon is increased or decreased?\n",
      "  * Increasing epsilon -> more exploration, Q-table learns more varied outcomes but converges slower.\n",
      "  * Decreasing epsilon -> more exploitation, policy stabilizes faster but might get stuck in suboptimal behavior.\n",
      "\n",
      "- How does Q-table change over time?\n",
      "  * Early training: small values and noisy preferences.\n",
      "  * Later: Q-values for good actions (GREEN in heavy, RED in empty) grow larger; bad actions get lower values.\n",
      "\n",
      "(You can re-run training with different hyperparameters to observe changes.)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Analysis ===\")\n",
    "print(\"- Does the system keep GREEN for heavy traffic?\")\n",
    "heavy_preferred = \"GREEN\" if Q[3,0] > Q[3,1] and Q[4,0] > Q[4,1] else \"not consistently GREEN\"\n",
    "print(f\"  * For heavy/very heavy (states 3 & 4), policy is {heavy_preferred}.\")\n",
    "print(f\"  * Q-values for heavy states: state 3 -> {np.round(Q[3],3)}, state 4 -> {np.round(Q[4],3)}\")\n",
    "\n",
    "print(\"\\n- Does it turn RED when the road is empty?\")\n",
    "empty_preferred = \"RED\" if Q[0,1] > Q[0,0] else \"not consistently RED\"\n",
    "print(f\"  * For empty (state 0), policy is {empty_preferred}.\")\n",
    "print(f\"  * Q-values for empty state: {np.round(Q[0],3)}\")\n",
    "\n",
    "print(\"\\n- What happens when epsilon is increased or decreased?\")\n",
    "print(\"  * Increasing epsilon -> more exploration, Q-table learns more varied outcomes but converges slower.\")\n",
    "print(\"  * Decreasing epsilon -> more exploitation, policy stabilizes faster but might get stuck in suboptimal behavior.\")\n",
    "\n",
    "print(\"\\n- How does Q-table change over time?\")\n",
    "print(\"  * Early training: small values and noisy preferences.\")\n",
    "print(\"  * Later: Q-values for good actions (GREEN in heavy, RED in empty) grow larger; bad actions get lower values.\")\n",
    "print(\"\\n(You can re-run training with different hyperparameters to observe changes.)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
